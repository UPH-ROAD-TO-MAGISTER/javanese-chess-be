Title: The Mechanics and Heuristics of Javanese Chess

Team 2
Harryanto Gani (0167925008)
Steven Aldo Sutanto (0167925009)
Rama Dwiyantara Perkasa (0167925010)

---------------------------------
I. INTRODUCTION
---------------------------------

Catur Jawa (Javanese Chess) is a traditional Indonesian board game played on a 9×9 grid. Each player holds a deck of 18 cards, consisting of two sets of numbers from 1 to 9, differentiated by color (for example: red, green, blue, purple). The game is turn-based and supports up to four players, including one AI-controlled bot.

Core rules:
- Players take turns placing cards adjacent to previously placed cards.
- The goal is to form a sequence of four cards in a line (horizontal, vertical, or diagonal).
- A player may place a card on top of an existing card if the new card has a strictly higher value.
- The game starts at the middle of the board at coordinate (5,5).
- The game ends immediately if someone achieves four in a row.
- If nobody gets four in a row and all cards are used, the winner is decided by the highest adjacent line sum.

This mechanism combines:
- spatial reasoning,
- numerical strategy,
- dynamic interaction between players.

Despite its strategic depth and cultural significance, Catur Jawa remains largely unexplored in digital form or AI-enhanced form. A machine learning–driven bot for this game serves two goals:
1. Preserve cultural heritage in a playable, modernized format.
2. Introduce intelligent gameplay dynamics that adapt to human players.

The proposed innovation is to develop a bot capable of playing competitively against humans using adaptive strategies. The bot should learn gameplay patterns and make context-aware decisions. Approaches such as deep reinforcement learning have shown that AI agents can navigate complex, uncertain, adversarial environments and improve over time through self-play and experience (Zhao et al., 2024). Integrating similar ideas into Catur Jawa allows us to modernize the game without discarding its traditional essence.

Primary objective:
- Design and implement a machine learning–based bot for Catur Jawa that can adapt to different board states and player strategies.

The bot should be able to:
- evaluate spatial configurations on the 9×9 board,
- predict opponent responses,
- choose optimal actions based on both card value and placement rules.

Secondary objectives:
- Compare the performance of different artificial intelligence (AI) and machine learning (ML) approaches in turn-based settings:
  • supervised learning,
  • reinforcement learning,
  • Monte Carlo Tree Search (MCTS).
- Study how AI changes:
  • player engagement,
  • fairness,
  • strategic variety.

Relevant prior work:
- Kowalski & Miernik (2023) studied AI agents in strategy card game competitions, focusing on both rule-based and learning-based agents.
- Welch (2021) discussed practical AI design for turn-based strategy games using decision trees and heuristic evaluation.
- Springer (2020) described how policy networks can be trained to make good moves in grid-based environments.
- Monte Carlo Tree Search and reinforcement learning have successfully optimized move selection in complex board games with spatial constraints (ScienceDirect, 2021).

Gap in literature:
While AI has made major progress in standardized games (chess, Go, card battlers), traditional and culture-specific games like Catur Jawa are underrepresented. Catur Jawa is also structurally different:
- It combines card hierarchy (higher-value cards can overwrite),
- spatial adjacency constraints (must place next to existing cards),
- and multi-agent interaction on a shared board.

This project attempts to formalize Catur Jawa as an AI/ML environment and build a bot with strategic behavior, helping both AI research and cultural preservation.

---------------------------------
II. DESIGN METHODOLOGY
---------------------------------

2.1 Game State Design
----------------------

2.1.1 Overview

The game state is a structured snapshot of the full situation of play. It is designed to be:
- human-readable,
- machine-readable,
- suitable for AI analysis.

Important:
- The game state does NOT itself enforce rules like legality of moves, overwrite conditions, or win checking.
- Those rules are implemented as external functions that READ and UPDATE the game state.
- This separation (state vs. rules) is standard in AI game modeling because it improves modularity, debugging, and reproducibility.

2.1.2 Game State Components

The game state consists of the following components:

1. Active Player
   - The ID of the player whose turn it is.
   - Required to manage turns and to tell the bot “who am I simulating?”

2. Action Phase
   - The current phase of the turn flow, for example:
     • setup
     • draw
     • place
     • resolve
     • end turn
   - This helps logic branch correctly based on where we are in the turn.

3. Board (9×9 grid)
   - Each cell is either empty OR contains a “card object”.
   - A card object has:
     • owner (which player placed it),
     • value (1–9),
     • status (whether it’s overwritable, etc.).
   - This grid structure supports spatial reasoning, pattern detection, and scoring.
   - It is similar in spirit to state encodings used in games like Go or Hex.

4. Hands
   - For each player: list/set of the cards currently in their hand.
   - Defines that player’s immediate action space.

5. Decks
   - Remaining cards in each player’s deck.
   - This may be stored as:
     • a list,
     • a pointer into a shuffled list,
     • or a seed for reproducible randomness.

6. Discards / History
   - Cards that have been played or discarded.
   - Useful for reconstructing the game sequence,
     which in turn helps training offline AI
     and measuring strategic depth.

2.1.3 Why This Structure Matters

We keep this structured representation because:
- It enables traceability (“why did the bot do this?”).
- It allows expert validation of moves in hindsight.
- It gives us a clean base representation we can transform later into numeric features for ML.

For example, for machine learning:
- We might later convert this state into tensors, multi-channel matrices, or feature vectors suitable for:
  • Policy networks,
  • Value networks,
  • Search-based evaluation (like MCTS).

This mirrors how policy/value networks in board games are typically trained (Springer, 2020; ScienceDirect, 2021).

2.2 Game Engine Design
----------------------

The game engine runs the loop:
1. Read the current game state.
2. Enumerate all legal moves for the current player.
3. Select one move (human or AI decision).
4. Apply that move to update the state deterministically.
5. Check win/tie conditions.

The engine is designed as a Finite State Machine (FSM):
- FSMs are widely used in turn-based game logic because they make phase transitions explicit and testable.
- Typical phases:
  • Determine player order.
  • Place the very first card at the board’s center (5,5).
  • Repeated turns:
    - Player chooses a legal placement.
    - Placement may overwrite an opponent’s card, but NEVER their own card.
    - Placement must obey adjacency rules.
    - Player cannot “skip”.
  • After each move:
    - Update board, hand, deck, discard history.
    - Check win condition or tie-break condition.

Win conditions:
- A player immediately wins if they create 4 cards in a row
  (horizontal, vertical, or diagonal).
Tie-break condition:
- If all cards are used and nobody formed 4 in a row,
  we compare the highest contiguous line sum and pick the highest.

This approach:
- cleanly separates rules from display/graphics,
- supports replay and reproducibility,
- makes it easier to plug in either human or AI players,
- and follows common best practices in AI gameplay research.

[IMAGE PLACEHOLDER: Figure 1. "Catur Jawa Finite State Machine Diagram"]

2.3 Game Engine Mathematical Model
----------------------------------

We describe each board cell as part of a state tuple:

⟨x, y, p(x,y), n(x,y), v(x,y)⟩

Where:

1. (x, y)
   - Board coordinates.
   - Domain: (x, y) ∈ A × A
   - A = {1,2,3,4,5,6,7,8,9}

2. p(x,y)
   - The player occupying that cell.
   - Domain: p(x,y) ∈ {0,1,2,3,4}
     • p(x,y) = 0  → the cell is empty (no player)
     • p(x,y) ≠ 0  → the cell is occupied by player p(x,y)

3. n(x,y)
   - The numeric value of the card currently on that cell.
   - Domain: n(x,y) ∈ {0,1,2,…,9}
     • n(x,y) = 0  → no card placed
     • n(x,y) ≠ 0  → card with value n(x,y) is present

4. v(x,y)
   - The accessibility/replaceability state of that cell.
   - Domain: v(x,y) ∈ {0,1,2}
     • v(x,y) = 0 → position (x,y) is inaccessible
     • v(x,y) = 1 → position (x,y) is accessible, meaning a player may place a card there
     • v(x,y) = 2 → position (x,y) is already occupied, but can potentially be replaced by a strictly higher-value card

The value v(x,y) changes automatically based on game evolution:
- From 0 → 1 when a cell becomes adjacent to an existing card.
- From 1 → 2 when a card is actually placed there.
- From 2 → 0 if that cell now contains a card with value 9 (cannot be replaced anymore).

Initial condition:
- For all (x,y): state is (x,y, 0, 0, 0)
- Except the board center (5,5), which starts as (5,5, 0, 0, 1)
  meaning “accessible starting position.”

Neighborhood expansion rule (intuitive form):
- After a card is placed at some coordinate (p,q),
  all neighbors around (p,q) in the 8 surrounding directions
  become accessible if they were empty before.

Algorithm sketch to update v(x,y) after placing a card:
1. Let player w place a card with value n(x,y) at cell (x,y).
2. Set p(x,y) = w.
3. For q from −1 to 1:
     For p from −1 to 1:
       If n(x+p, y+q) == 0:
         then v(x+p, y+q) = 1   (now accessible)
4. If n(x,y) = 9:
       v(x,y) = 0   (cannot be overwritten anymore)
   else:
       v(x,y) = 2   (occupied and overwritable by strictly higher value)

Tree of possibilities:
- After the very first placement at (5,5), there are up to 8 surrounding cells that become accessible to the next player:
  (4,4), (5,4), (6,4),
  (4,5),        (6,5),
  (4,6), (5,6), (6,6)

Only one of these will actually be chosen next, creating a branching game tree of possible future positions.

There are at least three important core algorithms in this game:
1. Determine legal moves.
2. Update v(x,y) values (board accessibility / overwritability).
3. Compute heuristic values for AI decision-making.

LEGAL MOVE CHECK
----------------
Placing a card of value z by player w into cell (x,y) is LEGAL if ALL are true:
1. v(x,y) ≠ 0
   (the cell is accessible or overwritable),
2. n(x,y) < z
   (the new card must have STRICTLY higher value than the card already there; if the cell is empty then n(x,y)=0 so any >0 works),
3. p(x,y) ≠ w
   (you are not allowed to overwrite your OWN card).

In pseudo-code:

LegalMove(x, y, z, w):
    if (v(x,y) ≠ 0) AND (n(x,y) < z) AND (p(x,y) ≠ w):
        return TRUE
    else:
        return FALSE

This enforces:
- you cannot skip,
- you cannot overwrite yourself,
- you can only overwrite an opponent if you beat their value.

[IMAGE PLACEHOLDER: "Game tree expansion / branching example"]

2.4 Heuristic Function
----------------------

We design a heuristic function H(s,a) to score each possible action a in state s.
This gives the AI a way to choose “good” moves.

Definition:

H(s,a) =
    W₁ · f_win(s,a)
  + W₂ · f_threat_block(s,a)
  + W₃ · f_replace_value(s,a)
  + W₄ · f_block_path(s,a)
  + W₅ · f_build_alignment(s,a)
  + W₆ · f_card_cost(a)

Where:
- H(s,a)             = total heuristic score of taking action a in state s.
- Wₖ                 = weight for feature k.
- f_*                = feature functions that evaluate specific tactical or strategic factors.

The feature components are:

1. f_win(s,a)
   Detects if this move immediately wins (creates 4 in a row).

2. f_threat_block(s,a)
   Detects whether the opponent is about to win (they have 3 in a row),
   and whether this move blocks that threat.

3. f_replace_value(s,a)
   Scores the benefit of overwriting an opponent’s card.

4. f_block_path(s,a)
   Rewards moves that block the opponent’s formation path.

5. f_build_alignment(s,a)
   Rewards moves that extend or create 2-in-a-row or 3-in-a-row for the current player.

6. f_card_cost(a)
   Scores how “expensive” the played card is, encouraging resource management.

Each subcomponent is detailed below.

--------------------------------------
2.4.1 f_win(s,a): Detecting Winning Moves
--------------------------------------

A player wins if they have four of their own cards aligned in a straight line:
- horizontal,
- vertical,
- diagonal (main or anti-diagonal).

If action a produces such a 4-in-a-row for the current player P, then:
f_win(s,a) = 1
otherwise:
f_win(s,a) = 0

The heuristic contribution might be:
W₁ · f_win(s,a) = 10,000
(very large to force the AI to take a winning move if available).

To detect a 4-in-a-row at position (x,y) just placed by player P:
Check sequences of length 4 that include (x,y) in all directions:

Horizontal:
∃k ∈ {−3,−2,−1,0} such that
    for all i ∈ {0,1,2,3} :
        p(x + k + i, y) = P

Vertical:
∃k ∈ {−3,−2,−1,0} such that
    for all i ∈ {0,1,2,3} :
        p(x, y + k + i) = P

Main diagonal:
∃k ∈ {−3,−2,−1,0} such that
    for all i ∈ {0,1,2,3} :
        p(x + k + i, y + k + i) = P

Anti-diagonal:
∃k ∈ {−3,−2,−1,0} such that
    for all i ∈ {0,1,2,3} :
        p(x + k + i, y − k − i) = P

If any check passes → immediate win.

---------------------------------------------------------
2.4.2 f_threat_block(s,a): Blocking Opponent’s Immediate Threat
---------------------------------------------------------

We scan for opponent formations of length 3 in a row (which means they could win next turn).
Let E be the opponent.

We look for:
- Horizontal: 3 aligned opponent cards.
- Vertical:   3 aligned opponent cards.
- Diagonal:   3 aligned opponent cards.

Formally, for each direction we test:

Horizontal:
∃k ∈ {−2,−1,0} such that
    for all i ∈ {0,1,2} :
        p(x + k + i, y) = E

(similar for vertical, main diagonal, anti-diagonal with appropriate coordinate changes.)

If such a 3-aligned threat exists and action a either:
- places a card in the critical 4th spot,
- overwrites one of those 3 cards with a higher-value card,
- or interrupts the chain (especially in the midpoint),

then:
f_threat_block(s,a) = 1
else:
f_threat_block(s,a) = 0

Typical contribution:
W₂ · f_threat_block(s,a) = 200

This encourages emergency defense.

-------------------------------------------------
2.4.3 f_replace_value(s,a): Overwriting Opponent Cards
-------------------------------------------------

This feature measures how valuable it is to overwrite an opponent’s card with a higher-value card.

Intuition:
- Replacing an opponent’s card in the middle of their dangerous formation is very strong.
- Replacing a random opponent card (not part of a threat) is still useful, but less urgent.

We define:

If f_threat_block(s,a) = 1 (i.e. this is answering an active 3-in-a-row threat):
    f_replace_value(s,a) = 200 + f_midpoint_bonus(a)
Else if the action simply overwrites an opponent card (legal overwrite, but not urgent):
    f_replace_value(s,a) = 125
Else:
    f_replace_value(s,a) = 0

The midpoint bonus f_midpoint_bonus(a) is:
- 75 if we overwrite the MIDDLE of the opponent’s 3-card line,
- 50 if we overwrite at the EDGE of that formation,
- 0 otherwise.

For example:
- Opponent has (3,1), (3,2), (3,3), so (3,2) is the midpoint horizontally.
- If we overwrite (3,2), that is very high value.

This feature rewards aggressive, targeted disruption.

-------------------------------------------------
2.4.4 f_block_path(s,a): Blocking Enemy Paths
-------------------------------------------------

This feature checks if our move “cuts” the enemy’s line by occupying a critical empty space BETWEEN two of their cards.

We consider (x,y) where we plan to play a card. Let E be the opponent.

A block occurs, for example, in the horizontal direction if:
- p(x+1, y) = E
- p(x-1, y) = E
- and (x,y) is currently empty

Similar logic applies for:
- vertical,
- main diagonal,
- anti-diagonal.

If the move at (x,y) breaks such a path:
    f_block_path(s,a) = 1
else:
    f_block_path(s,a) = 0

Heuristic contribution:
- If it’s responding to an active threat: W₄ · f_block_path(s,a) = 100
- If it’s more general blocking (preventive): W₄ · f_block_path(s,a) = 70

This feature gives the AI defensive awareness so it can “cut off” enemy lines before they become 3-in-a-row or 4-in-a-row.

-------------------------------------------------
2.4.5 f_build_alignment(s,a): Building Our Own Lines
-------------------------------------------------

This feature rewards proactive offense.

If action a creates or extends:
- 3 aligned cards for us → score 100
- 2 aligned cards for us → score 50
- otherwise → 0

Directions checked:
- Horizontal: (x-1,y), (x,y), (x+1,y)
- Vertical:   (x,y-1), (x,y), (x,y+1)
- Main diagonal: (x-1,y-1), (x,y), (x+1,y+1)
- Anti-diagonal: (x-1,y+1), (x,y), (x+1,y-1)

Also partial alignments of length 2 (like placing next to our existing card) are counted.

Heuristic contribution example:
W₅ · f_build_alignment(s,a) = 50 or 100

This teaches the AI to “set up” future wins, not just react defensively.

-------------------------------------------------
2.4.6 f_card_cost(a): Card Value Management
-------------------------------------------------

This feature looks at the numeric value of the card we’re about to play, and judges whether that’s an efficient use of our hand.

Context matters:
1. Threat Response (enemy already threatening with 3 in a row):
   - Playing a HIGHER-VALUE card is GOOD.
   - We reward higher values because neutralizing the threat is critical.

2. Potential / developing threat (enemy not yet at 3 in a row):
   - We prefer using LOWER-VALUE cards,
     saving stronger cards for emergencies.

3. Smallest Card Bonus:
   - If the played card is the lowest-valued card in our (current) hand of 3 cards,
     we add a +60 bonus.
   - This encourages efficient hand cycling and resource conservation.

Example scoring table:

Card Value | Threat Response Score | Potential Threat Score | Smallest Card Bonus
-----------|-----------------------|------------------------|--------------------
1          | 20                    | 100                    | +60
2          | 30                    | 90                     | +60
3          | 40                    | 80                     | +60
4          | 50                    | 70                     | +60
5          | 60                    | 60                     | +60
6          | 70                    | 50                     | +60
7          | 80                    | 40                     | +60
8          | 90                    | 30                     | +60
9          | 100                   | 20                     | +60

Interpretation:
- In a real threat, using a 9 is very valuable (score 100).
- In a low-pressure situation, using a 1 is very efficient (score 100).
- Playing your smallest card now is rewarded with +60 bonus.

This feature creates nuanced hand-management behavior in the AI:
- Don’t waste high cards on low-impact moves,
- But DO spend high cards to stop an immediate loss.

---------------------------------
III. RESULT
---------------------------------

[Section placeholder: Experimental results, simulations, self-play evaluation, win rates, etc.]

(This section in the provided PDF does not yet contain detailed numeric results.)

---------------------------------
IV. CONCLUSION
---------------------------------

[Section placeholder: High-level summary, future work, evaluation of heuristic-driven AI vs. learned policy, etc.]

---------------------------------
REFERENCES
---------------------------------

ACM. (2023). Differentiable economics: Strategic behaviour, mechanisms, and machine learning. Communications of the ACM.

Kowalski, J., & Miernik, R. (2023). Summarizing strategy card game AI competition. arXiv.

ScienceDirect. (2021). Monte Carlo tree search and reinforcement learning in board games. Procedia Computer Science, 187, 45–52.

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484–489.

Springer. (2020). Designing policy networks with deep learning in turn-based strategy games. In Advances in Intelligent Systems and Computing (pp. 145–160).

Springer. (2020). Multi-agent reinforcement learning: A selective overview of theories and applications. In Multi-Agent Systems and Applications (pp. 201–220).

Springer. (2021). International Journal of Machine Learning and Cybernetics.

Springer. (2021). Understanding the interplay of artificial intelligence and strategic management. Journal of Business Economics.

Springer. (2023). Machine Learning and Artificial Intelligence.

Welch, E. (2021). Designing AI algorithms for turn-based strategy games. Game Developer.

Zhao, Y., Wang, J., & Liu, H. (2024). Deep reinforcement learning in real-time strategy games: A systematic review. Applied Intelligence.

---------------------------------
END OF TEXT
---------------------------------
